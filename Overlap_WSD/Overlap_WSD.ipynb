{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the following command in terminal to download GoogleNews dataset in the models folder we have made: <br/>\n",
    "## wget -c \"https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "model_w2v = KeyedVectors.load_word2vec_format('models/GoogleNews-vectors-negative300.bin.gz',binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import semcor\n",
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import time\n",
    "import re\n",
    "import warnings\n",
    "from sklearn import metrics\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating word vectors for Semcor data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vectors = {}\n",
    "semcor_sents = semcor.sents()\n",
    "for sentence in semcor_sents:\n",
    "    for word in sentence:\n",
    "        if word in model_w2v:\n",
    "            vectors[word] = model_w2v.get_vector(word)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to get a single vector for entire sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def sent_vec(sentence):\n",
    "    li = []\n",
    "    for word in sentence:\n",
    "        if word in vectors:\n",
    "            li.append(vectors[word])\n",
    "    if len(li) == 0:\n",
    "        return np.zeros(300)\n",
    "    return np.average(li,axis=0)\n",
    "\n",
    "tf_idf_model = TfidfVectorizer()\n",
    "tf_idf_model.fit([\" \".join(list(x)) for x in (semcor_sents)])\n",
    "idf_values = dict(zip(tf_idf_model.get_feature_names(),list(tf_idf_model.idf_)))\n",
    "\n",
    "\n",
    "def tf_idf_sent_vec(sentence):\n",
    "    sentence_vector = np.zeros(300)\n",
    "    total_weight = 0\n",
    "    for word in sentence:\n",
    "        if word in vectors:\n",
    "            if word in idf_values.keys():\n",
    "                tf_idf = idf_values[word.lower()]*(sentence.count(word)/len(sentence))\n",
    "            else:\n",
    "                tf_idf = 0\n",
    "            sentence_vector += (vectors[word] * tf_idf)\n",
    "            total_weight += tf_idf\n",
    "    if total_weight != 0:\n",
    "        return sentence_vector/total_weight\n",
    "    else:\n",
    "        return sentence_vector\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict_synset without TF_IDF with Extended Lesk approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "charge.v.17\n"
     ]
    }
   ],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def predict_synset_extended(word, sentence, label):\n",
    "    sentence = [w for w in sentence if (w not in stop_words) and (w.isalnum())]\n",
    "\n",
    "    if word in sentence:\n",
    "        sentence.remove(word)\n",
    "        \n",
    "    context_bag = sent_vec(sentence)\n",
    "    \n",
    "    \n",
    "    sense_bag = {}\n",
    "    senses = wn.synsets(word, pos = label)\n",
    "\n",
    "    if len(senses)>0:\n",
    "        for sense in senses:\n",
    "            sense_bag[sense] = []\n",
    "            sense_bag[sense].append(sent_vec([w for w in sense.definition().split() if (w not in stop_words) and (w.isalnum())]))\n",
    "\n",
    "            for hypo in sense.hyponyms():\n",
    "                sense_bag[sense].append(sent_vec([w for w in hypo.definition().split() if (w not in stop_words) and (w.isalnum())]))\n",
    "        \n",
    "        if len(sense_bag.keys()) > 0:\n",
    "            synset = \"\"\n",
    "\n",
    "            cos_sims = []\n",
    "            for key,val in sense_bag.items():\n",
    "                cos_sims.append((key,model_w2v.cosine_similarities(context_bag,val).mean()))#,model_w2v.cosine_similarities(context_bag,val)))\n",
    "            cos_sims.sort(key = lambda x: x[1],reverse=True)\n",
    "            return str(cos_sims[0][0])[8:-2]\n",
    "    else:\n",
    "        return \"NA\"\n",
    "\n",
    "sentence = ['The', 'in', 'that', 'the', 'City', 'Executive', 'Committee', ',', 'which', 'over-all', 'charge', 'of', 'the', 'election', ',', '``', 'deserves', 'the', 'praise', 'and', 'thanks', 'of', 'the', 'City', 'of', 'Atlanta', \"''\", 'for', 'the', 'manner', 'in', 'which', 'the', 'election', 'was', 'conducted', '.']\n",
    "print(predict_synset_extended(\"charge\",sentence,'v'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict_synset without TF_IDF with Lesk approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "charge.v.02\n"
     ]
    }
   ],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def predict_synset(word, sentence, label):\n",
    "    sentence = [w for w in sentence if (w not in stop_words) and (w.isalnum())]\n",
    "    \n",
    "    if word in sentence:\n",
    "        sentence.remove(word)\n",
    "        \n",
    "    context_bag = sent_vec(sentence)\n",
    "        \n",
    "    sense_bag = []\n",
    "    senses = wn.synsets(word, pos = label)\n",
    "    \n",
    "    if len(senses)>0:\n",
    "        max_cosine_similarity = 0\n",
    "        synset = \"\"\n",
    "        i = 0 \n",
    "        for sense in senses:\n",
    "            sense_bag.append(sent_vec([w for w in sense.definition().split() if (w not in stop_words) and (w.isalnum())]))   \n",
    "            i += 1\n",
    "\n",
    "        cosine_similarities = model_w2v.cosine_similarities(context_bag,sense_bag)\n",
    "        max_cosine_similarity = 0\n",
    "        synset = \"\"\n",
    "        for i in range(len(cosine_similarities)):\n",
    "            if(max_cosine_similarity < cosine_similarities[i]):\n",
    "                max_cosine_similarity = cosine_similarities[i]\n",
    "                synset = senses[i]\n",
    "        return str(synset)[8:-2]\n",
    "    else:\n",
    "        return \"NA\"\n",
    "\n",
    "sentence = ['The', 'in', 'that', 'the', 'City', 'Executive', 'Committee', ',', 'which', 'over-all', 'charge', 'of', 'the', 'election', ',', '``', 'deserves', 'the', 'praise', 'and', 'thanks', 'of', 'the', 'City', 'of', 'Atlanta', \"''\", 'for', 'the', 'manner', 'in', 'which', 'the', 'election', 'was', 'conducted', '.']\n",
    "print(predict_synset(\"charge\",sentence,'v'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict_synset with TF_IDF Lesk approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "charge.v.02\n"
     ]
    }
   ],
   "source": [
    "def predict_synset_with_tf_idf(word, sentence, label):\n",
    "    sentence = [w for w in sentence if (w not in stop_words) and (w.isalnum())]\n",
    "    \n",
    "    if word in sentence:\n",
    "        sentence.remove(word)\n",
    "    context_bag = tf_idf_sent_vec(sentence)\n",
    "    sense_bag = []\n",
    "    senses = wn.synsets(word, pos = label)\n",
    "    \n",
    "    if len(senses)>0:\n",
    "        max_cosine_similarity = 0\n",
    "        synset = \"\"\n",
    "        i = 0 \n",
    "        for sense in senses:\n",
    "            sense_bag.append(tf_idf_sent_vec([w for w in sense.definition().split() if (w not in stop_words) and (w.isalnum())]))   \n",
    "            i += 1\n",
    "\n",
    "        cosine_similarities = model_w2v.cosine_similarities(context_bag,sense_bag)\n",
    "        max_cosine_similarity = 0\n",
    "        synset = \"\"\n",
    "        for i in range(len(cosine_similarities)):\n",
    "            if(max_cosine_similarity < cosine_similarities[i]):\n",
    "                max_cosine_similarity = cosine_similarities[i]\n",
    "                synset = senses[i]\n",
    "        return str(synset)[8:-2]\n",
    "    else:\n",
    "        return \"NA\"\n",
    "\n",
    "sentence = ['The', 'in', 'that', 'the', 'City', 'Executive', 'Committee', ',', 'which', 'over-all', 'charge', 'of', 'the', 'election', ',', '``', 'deserves', 'the', 'praise', 'and', 'thanks', 'of', 'the', 'City', 'of', 'Atlanta', \"''\", 'for', 'the', 'manner', 'in', 'which', 'the', 'election', 'was', 'conducted', '.']\n",
    "print(predict_synset_with_tf_idf(\"charge\",sentence,\"v\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict_synset with TF_IDF with Extended Lesk approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "charge.v.17\n"
     ]
    }
   ],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def predict_synset_with_tf_idf_extended(word, sentence, label):\n",
    "    sentence = [w for w in sentence if (w not in stop_words) and (w.isalnum())]\n",
    "\n",
    "    if word in sentence:\n",
    "        sentence.remove(word)\n",
    "        \n",
    "    context_bag = tf_idf_sent_vec(sentence)\n",
    "\n",
    "    sense_bag = {}\n",
    "    senses = wn.synsets(word, pos = label)\n",
    "\n",
    "    if len(senses)>0:\n",
    "        for sense in senses:\n",
    "            sense_bag[sense] = []\n",
    "            sense_bag[sense].append(tf_idf_sent_vec([w for w in sense.definition().split() if (w not in stop_words) and (w.isalnum())]))\n",
    "\n",
    "            for hypo in sense.hyponyms():\n",
    "                sense_bag[sense].append(tf_idf_sent_vec([w for w in hypo.definition().split() if (w not in stop_words) and (w.isalnum())]))\n",
    "\n",
    "        if len(sense_bag.keys()) > 0:\n",
    "            synset = \"\"\n",
    "\n",
    "            cos_sims = []\n",
    "            for key,val in sense_bag.items():\n",
    "                cos_sims.append((key,model_w2v.cosine_similarities(context_bag,val).mean()))#,model_w2v.cosine_similarities(context_bag,val)))\n",
    "            cos_sims.sort(key = lambda x: x[1],reverse=True)\n",
    "            return str(cos_sims[0][0])[8:-2]\n",
    "    else:\n",
    "        return \"NA\"\n",
    "\n",
    "sentence = ['The', 'in', 'that', 'the', 'City', 'Executive', 'Committee', ',', 'which', 'over-all', 'charge', 'of', 'the', 'election', ',', '``', 'deserves', 'the', 'praise', 'and', 'thanks', 'of', 'the', 'City', 'of', 'Atlanta', \"''\", 'for', 'the', 'manner', 'in', 'which', 'the', 'election', 'was', 'conducted', '.']\n",
    "print(predict_synset_extended(\"charge\",sentence,'v'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting sysnets for entire data without TF_IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "425.95264196395874\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "max=0\n",
    "words = []\n",
    "actual_synsets = []\n",
    "predicted_synsets = []\n",
    "predicted_synsets_extended = []\n",
    "\n",
    "i=0\n",
    "j=0\n",
    "semcor_tagged_sents = semcor.tagged_sents(tag='sem')\n",
    "for sent in semcor_tagged_sents[:] :\n",
    "    sentence = semcor_sents[i]\n",
    "    i += 1\n",
    "    for word in sent:\n",
    "        if type(word) == list:\n",
    "            continue\n",
    "        if (word.height() == 3):\n",
    "            max = word.height() \n",
    "            if str(word[0])[1:3] == \"NE\":\n",
    "                continue\n",
    "\n",
    "        elif ((word.height() == 2) and str(word.label()) != \"NE\"):\n",
    "            words.extend(word.leaves())\n",
    "            if not isinstance(word.label(), str) :\n",
    "                actual_synsets.extend([word.label().synset().name() for x in word.leaves()])\n",
    "            else:\n",
    "                actual_synsets.extend([word.label() for x in word.leaves()])\n",
    "\n",
    "            for x in word.leaves():\n",
    "                    predicted_synsets.append(predict_synset(x,sentence, re.findall('.*\\.([asrnv])\\..*',str(word.label()))[0]))\n",
    "                    predicted_synsets_extended.append(predict_synset_extended(x,sentence, re.findall('.*\\.([asrnv])\\..*',str(word.label()))[0]))\n",
    "                    j += 1\n",
    "\n",
    "print(time.time() - start)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extended Lesk without TF_IDF Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TP=89445\tTOTAL=240415\tAccuracy=0.37204417361645487\n",
      "Precision, Recall, F1 Score (0.3646944860867053, 0.3588727112118985, 0.33204431721211286, None)\n",
      "Precision, Recall, F1 Score (0.37204417361645487, 0.37204417361645487, 0.37204417361645487, None)\n",
      "Precision, Recall, F1 Score (0.5298931771302642, 0.37204417361645487, 0.3710714289336667, None)\n"
     ]
    }
   ],
   "source": [
    "true_pos = 0\n",
    "total = 0\n",
    "for i in range(len(actual_synsets)):\n",
    "    total += 1\n",
    "    if actual_synsets[i]==predicted_synsets_extended[i]:\n",
    "        true_pos += 1\n",
    "print(\"TP={}\\tTOTAL={}\\tAccuracy={}\".format(true_pos,total,true_pos/total))\n",
    "print(\"Precision, Recall, F1 Score\",metrics.precision_recall_fscore_support(actual_synsets, predicted_synsets_extended,average=\"macro\"))\n",
    "print(\"Precision, Recall, F1 Score\",metrics.precision_recall_fscore_support(actual_synsets, predicted_synsets_extended,average=\"micro\"))\n",
    "print(\"Precision, Recall, F1 Score\",metrics.precision_recall_fscore_support(actual_synsets, predicted_synsets_extended,average=\"weighted\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesk without TF_IDF Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TP=82599\tTOTAL=240415\tAccuracy=0.34356841295260276\n",
      "Precision, Recall, F1 Score (0.35864556702979183, 0.3537667531101894, 0.32804743922137636, None)\n",
      "Precision, Recall, F1 Score (0.34356841295260276, 0.34356841295260276, 0.34356841295260276, None)\n",
      "Precision, Recall, F1 Score (0.5133966728897991, 0.34356841295260276, 0.3496021300937267, None)\n"
     ]
    }
   ],
   "source": [
    "true_pos = 0\n",
    "total = 0\n",
    "for i in range(len(actual_synsets)):\n",
    "    total += 1\n",
    "    if actual_synsets[i]==predicted_synsets[i]:\n",
    "        true_pos += 1\n",
    "print(\"TP={}\\tTOTAL={}\\tAccuracy={}\".format(true_pos,total,true_pos/total))\n",
    "print(\"Precision, Recall, F1 Score\",metrics.precision_recall_fscore_support(actual_synsets, predicted_synsets,average=\"macro\"))\n",
    "print(\"Precision, Recall, F1 Score\",metrics.precision_recall_fscore_support(actual_synsets, predicted_synsets,average=\"micro\"))\n",
    "print(\"Precision, Recall, F1 Score\",metrics.precision_recall_fscore_support(actual_synsets, predicted_synsets,average=\"weighted\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting sysnets for entire data with TF_IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2309.4560351371765\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "max=0\n",
    "words_with_tf_idf = []\n",
    "actual_synsets_with_tf_idf = []\n",
    "predicted_synsets_with_tf_idf = []\n",
    "predicted_synsets_with_tf_idf_extended = []\n",
    "\n",
    "i=0\n",
    "j=0\n",
    "for sent in semcor_tagged_sents[:int(len(semcor_tagged_sents)*0.1)]:\n",
    "    sentence = semcor.sents()[i]\n",
    "    i += 1\n",
    "    for word in sent:\n",
    "        if type(word) == list:\n",
    "            continue\n",
    "        if (word.height() == 3):\n",
    "            max = word.height() \n",
    "            if str(word[0])[1:3] == \"NE\":\n",
    "                continue\n",
    "\n",
    "        elif ((word.height() == 2) and str(word.label()) != \"NE\"):\n",
    "                words_with_tf_idf.extend(word.leaves())\n",
    "                if not isinstance(word.label(), str) :\n",
    "                    actual_synsets_with_tf_idf.extend([word.label().synset().name() for x in word.leaves()])\n",
    "                else:\n",
    "                    actual_synsets_with_tf_idf.extend([word.label() for x in word.leaves()])\n",
    "\n",
    "                for x in word.leaves():\n",
    "                        predicted_synsets_with_tf_idf.append(predict_synset_with_tf_idf(x,sentence, re.findall('.*\\.([asrnv])\\..*',str(word.label()))[0]))\n",
    "                        predicted_synsets_with_tf_idf_extended.append(predict_synset_with_tf_idf_extended(x,sentence, re.findall('.*\\.([asrnv])\\..*',str(word.label()))[0]))\n",
    "                        j += 1\n",
    "\n",
    "print(time.time() - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesk with Tf-Idf values results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TP=16947\tTOTAL=43529\tAccuracy=0.38932665579268994\n",
      "Precision, Recall, F1 Score (0.31789091914840145, 0.30383029489680646, 0.2921225720181405, None)\n",
      "Precision, Recall, F1 Score (0.38932665579268994, 0.38932665579268994, 0.38932665579268994, None)\n",
      "Precision, Recall, F1 Score (0.538804069693409, 0.38932665579268994, 0.4064881152938287, None)\n"
     ]
    }
   ],
   "source": [
    "true_pos = 0\n",
    "total = 0\n",
    "for i in range(len(actual_synsets_with_tf_idf)):\n",
    "    total += 1\n",
    "    if actual_synsets_with_tf_idf[i]==predicted_synsets_with_tf_idf[i]:\n",
    "        true_pos += 1\n",
    "print(\"TP={}\\tTOTAL={}\\tAccuracy={}\".format(true_pos,total,true_pos/total))\n",
    "print(\"Precision, Recall, F1 Score\",metrics.precision_recall_fscore_support(actual_synsets_with_tf_idf, predicted_synsets_with_tf_idf,average=\"macro\"))\n",
    "print(\"Precision, Recall, F1 Score\",metrics.precision_recall_fscore_support(actual_synsets_with_tf_idf, predicted_synsets_with_tf_idf,average=\"micro\"))\n",
    "print(\"Precision, Recall, F1 Score\",metrics.precision_recall_fscore_support(actual_synsets_with_tf_idf, predicted_synsets_with_tf_idf,average=\"weighted\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extended Lesk with Tf-Idf values results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TP=18065\tTOTAL=43529\tAccuracy=0.41501068253348344\n",
      "Precision, Recall, F1 Score (0.3212415951340641, 0.3065779458714964, 0.29455392045190104, None)\n",
      "Precision, Recall, F1 Score (0.41501068253348344, 0.41501068253348344, 0.41501068253348344, None)\n",
      "Precision, Recall, F1 Score (0.5490601696657545, 0.41501068253348344, 0.4251691901585739, None)\n"
     ]
    }
   ],
   "source": [
    "true_pos = 0\n",
    "total = 0\n",
    "for i in range(len(actual_synsets_with_tf_idf)):\n",
    "    total += 1\n",
    "    if actual_synsets_with_tf_idf[i]==predicted_synsets_with_tf_idf_extended[i]:\n",
    "        true_pos += 1\n",
    "print(\"TP={}\\tTOTAL={}\\tAccuracy={}\".format(true_pos,total,true_pos/total))\n",
    "print(\"Precision, Recall, F1 Score\",metrics.precision_recall_fscore_support(actual_synsets_with_tf_idf, predicted_synsets_with_tf_idf_extended,average=\"macro\"))\n",
    "print(\"Precision, Recall, F1 Score\",metrics.precision_recall_fscore_support(actual_synsets_with_tf_idf, predicted_synsets_with_tf_idf_extended,average=\"micro\"))\n",
    "print(\"Precision, Recall, F1 Score\",metrics.precision_recall_fscore_support(actual_synsets_with_tf_idf, predicted_synsets_with_tf_idf_extended,average=\"weighted\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to check for individual test cases:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_ad_hoc(word,sentence,pos_tag):\n",
    "    print(\"Sentence = \",sentence)\n",
    "    print(\"Lesk Result = \",predict_synset(word,sentence.split(\" \"),pos_tag))\n",
    "    print(\"Extended Lesk Result = \",predict_synset_extended(word,sentence.split(\" \"),pos_tag))\n",
    "    print(\"Lesk with tf-idf sentence vectorizer Result = \",predict_synset_with_tf_idf(word,sentence.split(\" \"),pos_tag))\n",
    "    print(\"Extended Lesk tf-idf sentence vectorizer Result = \",predict_synset_with_tf_idf_extended(word,sentence.split(\" \"),pos_tag),\"\\n\\n\")\n",
    "\n",
    "\n",
    "def print_all_gloss(word,pos_tag):\n",
    "    for gloss in wn.synsets(word,pos_tag):\n",
    "        print(str(gloss),\" = \",gloss.definition())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence =  I went to the bank to withdraw some money.\n",
      "Lesk Result =  bank.n.07\n",
      "Extended Lesk Result =  bank.n.07\n",
      "Lesk with tf-idf sentence vectorizer Result =  bank.n.07\n",
      "Extended Lesk tf-idf sentence vectorizer Result =  bank.n.07 \n",
      "\n",
      "\n",
      "Sentence =  I went to the bank to deposit some money.\n",
      "Lesk Result =  depository_financial_institution.n.01\n",
      "Extended Lesk Result =  savings_bank.n.02\n",
      "Lesk with tf-idf sentence vectorizer Result =  depository_financial_institution.n.01\n",
      "Extended Lesk tf-idf sentence vectorizer Result =  depository_financial_institution.n.01 \n",
      "\n",
      "\n",
      "Sentence =  I went to the bank institution to withdraw some money.\n",
      "Lesk Result =  depository_financial_institution.n.01\n",
      "Extended Lesk Result =  depository_financial_institution.n.01\n",
      "Lesk with tf-idf sentence vectorizer Result =  depository_financial_institution.n.01\n",
      "Extended Lesk tf-idf sentence vectorizer Result =  depository_financial_institution.n.01 \n",
      "\n",
      "\n",
      "Sentence =  I went to the bank to have a bath in the river.\n",
      "Lesk Result =  savings_bank.n.02\n",
      "Extended Lesk Result =  bank.n.07\n",
      "Lesk with tf-idf sentence vectorizer Result =  bank.n.01\n",
      "Extended Lesk tf-idf sentence vectorizer Result =  bank.n.01 \n",
      "\n",
      "\n",
      "Sentence =  I went to the bank to have a bath in the river body.\n",
      "Lesk Result =  bank.n.01\n",
      "Extended Lesk Result =  bank.n.01\n",
      "Lesk with tf-idf sentence vectorizer Result =  bank.n.01\n",
      "Extended Lesk tf-idf sentence vectorizer Result =  bank.n.01 \n",
      "\n",
      "\n",
      "Sentence =  The school is seeing return of students.\n",
      "Lesk Result =  school.n.02\n",
      "Extended Lesk Result =  school.n.02\n",
      "Lesk with tf-idf sentence vectorizer Result =  school.n.02\n",
      "Extended Lesk tf-idf sentence vectorizer Result =  school.n.02 \n",
      "\n",
      "\n",
      "Sentence =  The school of fish is swimming past the island.\n",
      "Lesk Result =  school.n.07\n",
      "Extended Lesk Result =  school.n.07\n",
      "Lesk with tf-idf sentence vectorizer Result =  school.n.07\n",
      "Extended Lesk tf-idf sentence vectorizer Result =  school.n.07 \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predict_ad_hoc(\"bank\", \"I went to the bank to withdraw some money.\", \"n\")\n",
    "predict_ad_hoc(\"bank\", \"I went to the bank to deposit some money.\", \"n\")\n",
    "predict_ad_hoc(\"bank\", \"I went to the bank institution to withdraw some money.\", \"n\")\n",
    "\n",
    "predict_ad_hoc(\"bank\", \"I went to the bank to have a bath in the river.\", \"n\")\n",
    "predict_ad_hoc(\"bank\", \"I went to the bank to have a bath in the river body.\", \"n\")\n",
    "\n",
    "predict_ad_hoc(\"school\", \"The school is seeing return of students.\", \"n\")\n",
    "\n",
    "predict_ad_hoc(\"school\", \"The school of fish is swimming past the island.\", \"n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synset('bank.n.01')  =  sloping land (especially the slope beside a body of water)\n",
      "Synset('depository_financial_institution.n.01')  =  a financial institution that accepts deposits and channels the money into lending activities\n",
      "Synset('bank.n.03')  =  a long ridge or pile\n",
      "Synset('bank.n.04')  =  an arrangement of similar objects in a row or in tiers\n",
      "Synset('bank.n.05')  =  a supply or stock held in reserve for future use (especially in emergencies)\n",
      "Synset('bank.n.06')  =  the funds held by a gambling house or the dealer in some gambling games\n",
      "Synset('bank.n.07')  =  a slope in the turn of a road or track; the outside is higher than the inside in order to reduce the effects of centrifugal force\n",
      "Synset('savings_bank.n.02')  =  a container (usually with a slot in the top) for keeping money at home\n",
      "Synset('bank.n.09')  =  a building in which the business of banking transacted\n",
      "Synset('bank.n.10')  =  a flight maneuver; aircraft tips laterally about its longitudinal axis (especially in turning)\n"
     ]
    }
   ],
   "source": [
    "print_all_gloss(\"bank\",\"n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synset('school.n.01')  =  an educational institution\n",
      "Synset('school.n.02')  =  a building where young people receive education\n",
      "Synset('school.n.03')  =  the process of being formally educated at a school\n",
      "Synset('school.n.04')  =  a body of creative artists or writers or thinkers linked by a similar style or by similar teachers\n",
      "Synset('school.n.05')  =  the period of instruction in a school; the time period when school is in session\n",
      "Synset('school.n.06')  =  an educational institution's faculty and students\n",
      "Synset('school.n.07')  =  a large group of fish\n"
     ]
    }
   ],
   "source": [
    "print_all_gloss(\"school\",\"n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence =  The water overflowed the bank rooms filled with money.\n",
      "Lesk Result =  savings_bank.n.02\n",
      "Extended Lesk Result =  bank.n.01\n",
      "Lesk with tf-idf sentence vectorizer Result =  bank.n.01\n",
      "Extended Lesk tf-idf sentence vectorizer Result =  bank.n.01 \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predict_ad_hoc(\"bank\", \"The water overflowed the bank rooms filled with money.\", \"n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence =  I went to a bank which is situated on the river bank.\n",
      "Lesk Result =  bank.n.01\n",
      "Extended Lesk Result =  bank.n.01\n",
      "Lesk with tf-idf sentence vectorizer Result =  bank.n.01\n",
      "Extended Lesk tf-idf sentence vectorizer Result =  bank.n.01 \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predict_ad_hoc(\"bank\", \"I went to a bank which is situated on the river bank.\", \"n\")\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
